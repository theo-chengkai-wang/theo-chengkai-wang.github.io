---
title: "Reverse engineering the Acronym task in GPT-2 small (WIP)"
collection: works
category: side
permalink: /works/2024-09-aisf
excerpt: "AISF project, based on one of Neel Nanda's 200 open problems in Mechanistic Interpretability."
date: 2024-09-29
# venue: 'Thesis'
label: 'AISF Project'
# urls: [{name: "Extended Abstract", url: "/files/works/2023_11_Lys_extended_abstract-11.pdf"}, 
#         {name: "Poster", url: "/files/works/2024_01_Oxford_Poster_Lys-7.pdf"}]
# slidesurl: 'http://academicpages.github.io/files/slides1.pdf'
# paperurl: 'http://academicpages.github.io/files/paper1.pdf'
# citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
---

This project is based on one of the suggested projects in Neel Nanda's [200 open problems in mechanistic interpretability](https://www.alignmentforum.org/s/yivyHaCAmMJ3CqSyj/p/LbrPTJ4fmABEdEnLf), completed for the Bluedot Impact's [AI safety fundamentals](https://aisafetyfundamentals.com) course on AI alignment.

We attempt to reverse engineer the task of predicting the 3-letter acronym for a 3-word sequence in GPT-2 small. We isolate some interesting attention heads and MLP layers, and make some hypotheses about how they work. We leave more rigorous explorations to future work.
